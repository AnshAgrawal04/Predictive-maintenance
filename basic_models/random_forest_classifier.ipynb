{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temporora import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: FD001\n",
      "Dataset FD001 Metrics:\n",
      "Best Hyperparameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 5}\n",
      "Test - Accuracy: 0.9095, Precision: 0.9444, Recall: 0.6800, F1 Score: 0.7907, ROC-AUC: 0.8333\n",
      "Processing dataset: FD002\n",
      "Dataset FD002 Metrics:\n",
      "Best Hyperparameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 5}\n",
      "Test - Accuracy: 0.8737, Precision: 0.8646, Recall: 0.5533, F1 Score: 0.6748, ROC-AUC: 0.7632\n",
      "Processing dataset: FD003\n",
      "Dataset FD003 Metrics:\n",
      "Best Hyperparameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 5}\n",
      "Test - Accuracy: 0.9520, Precision: 0.9419, Recall: 0.8100, F1 Score: 0.8710, ROC-AUC: 0.8988\n",
      "Processing dataset: FD004\n",
      "Dataset FD004 Metrics:\n",
      "Best Hyperparameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 5}\n",
      "Test - Accuracy: 0.8043, Precision: 0.6667, Recall: 0.2000, F1 Score: 0.3077, ROC-AUC: 0.5861\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming metrics is defined before this loop\n",
    "for dataset in datasets:\n",
    "    print(f\"Processing dataset: {dataset}\")\n",
    "    args.sub_dataset = dataset  # Set current dataset\n",
    "\n",
    "    # Load data for training, validation, and testing\n",
    "    train_loader, valid_loader, test_loader, test_loader_last, \\\n",
    "        num_test_windows, train_visualize, engine_id = get_dataloader(\n",
    "            dir_path=args.dataset_root,\n",
    "            sub_dataset=args.sub_dataset,\n",
    "            max_rul=args.max_rul,\n",
    "            seq_length=args.sequence_len,\n",
    "            batch_size=args.batch_size,\n",
    "            use_exponential_smoothing=args.use_exponential_smoothing,\n",
    "            smooth_rate=args.smooth_rate)\n",
    "\n",
    "    # Directly use the specified hyperparameters\n",
    "    best_params = {\n",
    "        'n_estimators': 200,\n",
    "        'max_depth': 20,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 5\n",
    "    }\n",
    "\n",
    "    # Train and evaluate using the specified hyperparameters\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=best_params[\"n_estimators\"],\n",
    "        max_depth=best_params[\"max_depth\"],\n",
    "        min_samples_split=best_params[\"min_samples_split\"],\n",
    "        min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Prepare training data\n",
    "    train_x, train_y = [], []\n",
    "    for x, y in train_loader:\n",
    "        train_x.append(x.view(-1, args.sequence_len * args.feature_num).numpy())\n",
    "        train_y.append((y.numpy() < 0.24).astype(int))  # Convert y to binary\n",
    "\n",
    "    # Convert lists to arrays\n",
    "    train_x = np.vstack(train_x)  # Stack along rows\n",
    "    train_y = np.concatenate(train_y, axis=0)  # Concatenate along rows\n",
    "\n",
    "    # Ensure y is a 1D array\n",
    "    train_y = train_y.ravel()\n",
    "\n",
    "    # Fit the model\n",
    "    rf_classifier.fit(train_x, train_y)\n",
    "\n",
    "    # Test the model\n",
    "    test_predictions, test_actuals = [], []\n",
    "    for x, y in test_loader:\n",
    "        x = x.view(-1, args.sequence_len * args.feature_num).numpy()\n",
    "        y = (y.numpy() < 0.24).astype(int)\n",
    "        preds = rf_classifier.predict(x)\n",
    "        test_predictions.extend(preds)\n",
    "        test_actuals.extend(y)\n",
    "\n",
    "    test_predictions = np.array(test_predictions)\n",
    "    test_actuals = np.array(test_actuals)\n",
    "\n",
    "    # Compute Metrics\n",
    "    test_accuracy = accuracy_score(test_actuals, test_predictions)\n",
    "    test_precision = precision_score(test_actuals, test_predictions)\n",
    "    test_recall = recall_score(test_actuals, test_predictions)\n",
    "    test_f1 = f1_score(test_actuals, test_predictions)\n",
    "    test_roc_auc = roc_auc_score(test_actuals, test_predictions)\n",
    "\n",
    "    # Store metrics\n",
    "    metrics[dataset] = {\n",
    "        \"Best Hyperparameters\": best_params,\n",
    "        \"Test Accuracy\": test_accuracy,\n",
    "        \"Test Precision\": test_precision,\n",
    "        \"Test Recall\": test_recall,\n",
    "        \"Test F1 Score\": test_f1,\n",
    "        \"Test ROC-AUC\": test_roc_auc\n",
    "    }\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Dataset {dataset} Metrics:\")\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "    print(f\"Test - Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1 Score: {test_f1:.4f}, ROC-AUC: {test_roc_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
